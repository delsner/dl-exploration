{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 12175 characters, 72 unique.\n"
     ]
    }
   ],
   "source": [
    "# import text and extract chars\n",
    "text = open('fdp.txt', 'r').read()\n",
    "unique_chars = list(set(text))\n",
    "text_size, vocab_size = len(text), len(unique_chars)\n",
    "print('data has %d characters, %d unique.' % (text_size, vocab_size))\n",
    "char_to_ix = {ch:i for i,ch in enumerate(unique_chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of RNN\n",
    "hidden_size = 100\n",
    "seq_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(v):\n",
    "    '''\n",
    "    Convert character labels to one-hot variants\n",
    "    :param v - index in char_to_index of character to convert\n",
    "    :returns - 0-Vector with 1 at index v\n",
    "    '''\n",
    "    return np.eye(vocab_size)[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the network\n",
    "# start with parameters\n",
    "x = tf.placeholder(tf.float32, [None, vocab_size]) # 25x76 = seq_length x vocab_size\n",
    "y_in = tf.placeholder(tf.float32, [None, vocab_size]) # 25x76 = seq_length x vocab_size\n",
    "h_start = tf.placeholder(tf.float32, [1, hidden_size]) # 1x100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define architecture\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    h_state = h_start # initialize hidden state\n",
    "    y_out_all = [] # collect each of y values choosen by network\n",
    "    # split input of seq_length into single inputs x_in with t as timestep\n",
    "    for t, x_in in enumerate(tf.split(x, seq_length,0)):\n",
    "        if t > 0: \n",
    "            # reuse same variables within one iteration of training (one seq_length)\n",
    "            scope.reuse_variables()\n",
    "        Wxh = tf.get_variable('Wxh', [vocab_size, hidden_size], initializer=initializer)\n",
    "        Whh = tf.get_variable('Whh', [hidden_size, hidden_size], initializer=initializer)\n",
    "        Why = tf.get_variable('Why', [hidden_size, vocab_size], initializer=initializer)\n",
    "        bh = tf.get_variable('bh', [hidden_size], initializer=initializer)\n",
    "        by = tf.get_variable('by', [vocab_size], initializer=initializer)\n",
    "        \n",
    "        # forward-pass through first hidden layer\n",
    "        h_state = tf.tanh(tf.matmul(x_in, Wxh) + tf.matmul(h_state, Whh) + bh)\n",
    "        # forward-pass through output layer\n",
    "        y_out = tf.matmul(h_state, Why) + by\n",
    "        y_out_all.append(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store last hidden state and calculate loss of all predicted y_out_all\n",
    "h_last = h_state\n",
    "output_softmax = tf.nn.softmax(y_out_all[-1])  # only for sampling\n",
    "outputs = tf.concat(y_out_all, 0)\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=y_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minimize loss by using backprop and sgd through adam optimizer\n",
    "minimizer = tf.train.AdamOptimizer()\n",
    "grads_and_vars = minimizer.compute_gradients(loss)\n",
    "\n",
    "# prevent gradient vanishing and explosion\n",
    "grad_clipping = tf.constant(5.0, name='grad_clipping')\n",
    "clipped_grads_and_vars = []\n",
    "for grad, var in grads_and_vars:\n",
    "    clipped_grad = tf.clip_by_value(grad, -grad_clipping, grad_clipping)\n",
    "    clipped_grads_and_vars.append((clipped_grad, var))\n",
    "    \n",
    "updates = minimizer.apply_gradients(clipped_grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling from the network to see progress (in textual form)\n",
    "def sample_network():\n",
    "    sample_length = 200\n",
    "    start_ix = random.randint(0, len(text) - seq_length)\n",
    "    sample_seq_ix = [char_to_ix[ch] for ch in text[start_ix:start_ix + seq_length]]\n",
    "    ixes = []\n",
    "    sample_prev_state_val = np.copy(h_start_val)\n",
    "    \n",
    "    for t in range(sample_length):\n",
    "        sample_input_vals = one_hot(sample_seq_ix)\n",
    "        sample_output_softmax_val, sample_prev_state_val = sess.run([output_softmax, h_last], feed_dict={x: sample_input_vals, h_start: sample_prev_state_val})\n",
    "        ix = np.random.choice(range(vocab_size), p=sample_output_softmax_val.ravel())\n",
    "        ixes.append(ix)\n",
    "        sample_seq_ix = sample_seq_ix[1:] + [ix]\n",
    "    \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print('-----\\n %s \\n ----\\n' % txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run tensorflow session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set starting position in text and number of iterations\n",
    "position_in_text = 0\n",
    "number_of_iterations = 0\n",
    "total_iterations = 10000\n",
    "h_start_val = np.zeros([1,hidden_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, p: 0, loss: 3.195930\n",
      "-----\n",
      " htdn  oinen Uin äescheen.\n",
      "dun  Eübendr.\n",
      "Dmlegesa dsn sinn schlde 1ichiendn kZschldgen siit Sn.\n",
      "Znd ch-npil ian ernlgengen uans Denden venseie benden.\n",
      "Suwichtchuus en:tNmet EiölEnzBhndsnes daentenHwhnu \n",
      " ----\n",
      "\n",
      "iter: 500, p: 350, loss: 2.026916\n",
      "-----\n",
      " t men LaGüe Zaso hin nit dir “ugüfte-enft.\n",
      "Mir ge dit Gengen tibeiln fer gze  füe gen un ku sin Vorlen mnde Erropl sallen Mabeao san an kiulen Alfe foolenn In eln dan di.\n",
      "Dasoen sobwie Heut, ie  eund  \n",
      " ----\n",
      "\n",
      "iter: 1000, p: 700, loss: 1.772866\n",
      "-----\n",
      " lbden.\n",
      "Zndern\n",
      "olineln tändegein: Unierserfen.ichen ehSch ben ich mid eunderefennecht schumeder er:-Fue .unsenznbehten Alder soaden hatchenichtelng den sbeldermelann senebtn Zinden.\n",
      "Uäder undernelt aäw \n",
      " ----\n",
      "\n",
      "iter: 1500, p: 1050, loss: 1.934769\n",
      "-----\n",
      " u1ndern.\n",
      "Lis kapn.\n",
      "\f",
      "EUNO9NIS5HzO Firlgen Lah eltebthen schen Soe Lneserenn deremzhlren solaus worten Sichändase nerNes ein Sh\f",
      "umgt AU vom Foren.\n",
      "0Men BlTsch fen.\n",
      "Int dnischenPm?er komnen\n",
      "Westger edar  \n",
      " ----\n",
      "\n",
      "iter: 2000, p: 1400, loss: 2.081880\n",
      "-----\n",
      " e gaschee Bies.\n",
      "Wie Soust hezn mit er 4an.\n",
      "Dien solrschen kommen.\n",
      "DeensR Intscham din flen, tinze für INgz Eereen le\n",
      "schwelbem sehren mehrer.\n",
      "Dad ure sicht den gane öinechern gedeicho find eicht der g \n",
      " ----\n",
      "\n",
      "iter: 2500, p: 1750, loss: 1.293652\n",
      "-----\n",
      " n bendah me reuts sie nenhn, des kinhe Pelatas gert dam en\n",
      "eln-Scheler ender nechenn.\n",
      "Sier gekoIn.\n",
      "Dade mache Fleichestet: Sie Unio Deschreffn sir vo mutschpien zuf chalen worben.\n",
      "Dabem mefer worben h \n",
      " ----\n",
      "\n",
      "iter: 3000, p: 2100, loss: 1.236113\n",
      "-----\n",
      " otpute sich uhbeisgeitäes\n",
      "in zist mutr Cönderhre Schulen seie Sokwind en ause kenro dieglern daeunichenn de-Richälschen.\n",
      "Deen schzr öom en anzumgem\n",
      "eisArmet nich buld en gien.\n",
      "Daut gasdie Pueltelncchw \n",
      " ----\n",
      "\n",
      "iter: 3500, p: 2450, loss: 1.187340\n",
      "-----\n",
      " ein zband Für lal mür iherben sal habdernn hren Gelang GelzuL jeder kongrneg d?e dei\n",
      "Lini beinn Srund-Nacher-Cowln id Eunoplie? Vorlenn kann.\n",
      "Wie Schüler gllonn nin tin gnender\n",
      "So konnden Schulen.\n",
      "Wir \n",
      " ----\n",
      "\n",
      "iter: 4000, p: 2800, loss: 0.844030\n",
      "-----\n",
      " n viest.\n",
      "Cie Schwlen warlen eusongehre Zuft Lä dar rich nisieder belten Kindernichoraust sie die Schulei tie Polldnfundssollen\n",
      "yinm In Schulen.\n",
      "Schulen sing dif deschtirtaschundwerte besschän iit bis  \n",
      " ----\n",
      "\n",
      "iter: 4500, p: 3150, loss: 1.438160\n",
      "-----\n",
      " köinen schün aunrter in? Daun kanfen\n",
      "schufd.-Schultistischen Schwternen Unden ein sin siwer einied eitmmer jutkchelmehr wern.\n",
      "Wir wollen.\n",
      "Dast andererten min erbertufürei-rungeln Schumes acheiköinet.\n",
      " \n",
      " ----\n",
      "\n",
      "iter: 5000, p: 3500, loss: 2.157581\n",
      "-----\n",
      " en ust jeder für eis teuen.\n",
      "Danne san din schun tiche Int.\n",
      "\f",
      "gUna Lind aun chle nisle können.\n",
      "Wih ienter Länderselen wie Feine getz Eurom ein dann.\n",
      "\f",
      "MUHE FIMOG\n",
      "WENROENI No LoUnjedes Dint died ues die f \n",
      " ----\n",
      "\n",
      "iter: 5500, p: 3850, loss: 0.989205\n",
      "-----\n",
      " t.\n",
      "In verum beiche fen.tIn Schulenim\n",
      "ter ben.\n",
      "Data ist gut faben eiter icht an anderten.\n",
      "In Udaen meichteunk Arerenes Wis Gesn.\n",
      "„Schurerde-Irt.\n",
      "Wie wornen uer.\n",
      "Und zwersten.\n",
      "kamm hernehre.\n",
      "Jan aute se \n",
      " ----\n",
      "\n",
      "iter: 6000, p: 4200, loss: 0.952860\n",
      "-----\n",
      " dch idan sis Gesein Schuld.\n",
      "Und zute Menncht Und ne sit onnen.\n",
      "Dan machen iert Verasien.\n",
      "In einen kumm.\n",
      "Dauso kundebticht mist. Wal nicht.\n",
      "Darun.chen Swoschne \n",
      "ehrmm-t.\n",
      "Darur ternest.\n",
      "Id Möng beint.\n",
      "W \n",
      " ----\n",
      "\n",
      "iter: 6500, p: 4550, loss: 2.662700\n",
      "-----\n",
      " den vorerenn briebRet ant dar vorkenberanz\n",
      "ERZoplerofen.\n",
      "Zau d.\n",
      "Zicheint für gas Kind für\n",
      "Minzl worlten sone nicht man singen.\n",
      "In HInso ders gender\n",
      "terseiten.\n",
      "Ster, die kongen.\n",
      "Darus 2mpamer ietern mu \n",
      " ----\n",
      "\n",
      "iter: 7000, p: 4900, loss: 1.221902\n",
      "-----\n",
      " illeg fürfebeiter beisurgem\n",
      "Virlan-inemen Es einen Länderarfön.\n",
      "Daune Ambedarzu guch Gendan.\n",
      "Wie finden:\n",
      "Dasin:\n",
      "Man siet eun her Gelo auts iemein mis sokomar ioaben Kindes: Men asts-heter wolle ver Eu \n",
      " ----\n",
      "\n",
      "iter: 7500, p: 5250, loss: 0.956903\n",
      "-----\n",
      " len.\n",
      "\f",
      "EUNEOAANLEGNTwBe MNGUNzELAUNG schäfo dien an sinder: dass-mehren.\n",
      "DaschtÄmt, daes der Geldizeis\n",
      "Mendanf\n",
      "muhr ust den könninderen Stalles Geld.\n",
      "\f",
      "VUrd.\n",
      "USdaz So die Wis, de\n",
      "sollet kunben: Int sie  \n",
      " ----\n",
      "\n",
      "iter: 8000, p: 5600, loss: 0.553020\n",
      "-----\n",
      " \n",
      "Wir wellehre Mehrang beg un den in jedeus müchtwer find zues Infez eismen haben, wis tur Benolter finken metrum\n",
      "Bsospäelltmummen dausoliet geschein hellosmen: Elgen.\n",
      "Dauch winder gets-Nliein Sokaute  \n",
      " ----\n",
      "\n",
      "iter: 8500, p: 5950, loss: 0.977351\n",
      "-----\n",
      " neicht Monot.\n",
      "Ibmes hneun best heter für\n",
      "eim gein.\n",
      "Was: voflu dan kümmen, die aus eimennwoltechwer uer Gele hat.\n",
      "Daum\n",
      "wercht die Mehstiese Verwoples zum dee Mitden, das achun konders ahs Geseihmter ie \n",
      " ----\n",
      "\n",
      "iter: 9000, p: 6300, loss: 1.507409\n",
      "-----\n",
      " en, das aus is en wilden?\n",
      "Dibe Dssollen dism eit asg.\n",
      "\f",
      "AUS EuLeN.\n",
      "Dle dan Kisden amfen,\n",
      "wachune Intein eo iere defürt.\n",
      "\f",
      "EURBERISBGHZapz Willein.\n",
      "\f",
      "VEHPjIVERIPBEIOERN BEREHPUNI2HA Verwursez\n",
      "wer dar füf  \n",
      " ----\n",
      "\n",
      "iter: 9500, p: 6650, loss: 1.248812\n",
      "-----\n",
      " d sie Rechten.\n",
      "Es köml-Re Kieder aus eune Verantur fürer-Gele het-Rie Arbeinsitwer ume daon die Firten\n",
      "ster Berund est nichtein wolimin mahwer wicht in deu ichti-Ze.\n",
      "Die Aobeitsles\n",
      "GemaCienn man wer I \n",
      " ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop for going through document and see if end was reached if not train again\n",
    "while number_of_iterations < total_iterations:\n",
    "    if position_in_text + seq_length + 1 >= len(\n",
    "            text) or number_of_iterations == 0:\n",
    "        # reset value of hidden state\n",
    "        h_start_val = np.zeros([1, hidden_size])\n",
    "        position_in_text = 0\n",
    "\n",
    "    inputs = one_hot([\n",
    "        char_to_ix[ch]\n",
    "        for ch in text[position_in_text:position_in_text + seq_length]\n",
    "    ])\n",
    "    targets = one_hot([\n",
    "        char_to_ix[ch]\n",
    "        for ch in text[position_in_text + 1:position_in_text + seq_length + 1]\n",
    "    ])\n",
    "    h_start_val, loss_val, _ = sess.run([h_last, loss, updates], feed_dict={x:inputs, y_in:targets, h_start:h_start_val})\n",
    "    \n",
    "    # sample every 500 iterations\n",
    "    if number_of_iterations % 500 == 0:\n",
    "        print('iter: %d, p: %d, loss: %f' %(number_of_iterations, position_in_text, loss_val))\n",
    "        sample_network()\n",
    "        \n",
    "    position_in_text += seq_length\n",
    "    number_of_iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
